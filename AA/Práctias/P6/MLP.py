import numpy as np
import math

class MLP:

    """
    Constructor: Computes MLP.

    Args:
        inputLayer (int): size of input
        hiddenLayers (list of int): sizes of hidden layers.
        outputLayer (int): size of output layer
        seed (scalar): seed of the random numeric.
        epislom (scalar) : random initialization range. e.j: 1 = [-1..1], 2 = [-2,2]...
    """

    def __init__(self, inputLayer, hiddenLayers, outputLayer, seed=0, epislom=0.12):
        np.random.seed(seed)
        self.layers = [inputLayer] + hiddenLayers + [outputLayer]
        self.thetas = []

        for i in range(len(self.layers) - 1):
            # Inicialización de los pesos de forma aleatoria
            t = np.random.rand(self.layers[i+1], self.layers[i] + 1) * 2 * epislom - epislom
            print("Theta ",t.shape)
            self.thetas.append(t)

    """
    Reset the theta matrix created in the constructor by both theta matrix manually loaded.

    Args:
        thetas (list of array_like): Weights for each layer in the neural network.
    """
    def new_trained(self, thetas):
        self.thetas = thetas

    """
    Num elements in the training data. (private)

    Args:
        x (array_like): input data. 
    """
    def _size(self, x):
        return x.shape[0]

    """
    Computes the sigmoid function of z (private)

    Args:
        z (array_like): activation signal received by the layer.
    """
    def _sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    """
    Computes the sigmoid derivation of the activation (private)

    Args:
        a (array_like): activation received by the layer.
    """
    def _sigmoidPrime(self, a):
        return a * (1 - a)

    """
    Run the feedforward neural network step

    Args:
        x (array_like): input of the neural network.

    Return 
    ------
    activations (list of array_like): activation functions of each layer
    signals (list of array_like): signal functions of each layer
    """
    def feedforward(self, x):
        activations = [np.hstack([np.ones((self._size(x), 1)), x])]  # Agregar sesgo a la capa de entrada

        # Propagación hacia adelante
        for i, theta in enumerate(self.thetas):
            z = np.dot(activations[-1], theta.T)  # Cálculo de la señal (entrada ponderada)
            a = self._sigmoid(z)  # Aplicación de la función sigmoide

            if i < len(self.thetas) - 1:  # Para todas las capas excepto la última
                a = np.hstack([np.ones((a.shape[0], 1)), a])  # Agregar un 1 para el sesgo

            activations.append(a)  # Guardar las activaciones de la capa

        return activations


    """
    Computes only the cost of a previously generated output (private)

    Args:
        yPrime (array_like): output generated by neural network.
        y (array_like): output from the dataset
        lambda_ (scalar): regularization parameter

    Return 
    ------
    J (scalar): the cost.
    """
    def compute_cost(self, yPrime, y, lambda_):
        J = 0
        m = self._size(y)

        # Calculamos el coste de la función de coste
        cost = y * np.log(yPrime) + (1 - y) * np.log(1 - yPrime)
        J = -np.sum(cost)
        J = J/m

        # Calculamos la regularización L2 teniendo en cuenta lo anterior
        reg_cost = self._regularizationL2Cost(m, lambda_)
        J = J + reg_cost   
        return J


    """
    Get the class with highest activation value

    Args:
        a3 (array_like): output generated by neural network.

    Return 
    ------
    predictions (array_like): predicted classes.
    """
    def predict(self, a3):
        for i in range(a3.shape[0]):
            a3[i] = np.where(a3[i] == np.amax(a3[i]), 1, 0)
        return a3
    

    """
    Compute the gradients of both theta matrix parámeters and cost J

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        lambda_ (scalar): regularization.

	Return 
	------
	J: cost
    grad1, grad2: the gradient matrix (same shape than theta1 and theta2)
    """
    def compute_gradients(self, x, y, lambda_):
        activations = self.feedforward(x)
        deltas = [activations[-1] - y]

        # Backpropagation
        for i in range(len(self.thetas) - 1, 0, -1):
            if i == (len(self.thetas) - 1):
                delta = np.dot(deltas[0], self.thetas[i] )* self._sigmoidPrime(activations[i])
            else:
                delta = np.dot(deltas[0][:,1:], self.thetas[i] )* self._sigmoidPrime(activations[i])
            deltas.insert(0, delta)

        # Añadimos la regularizacion L2 a los gradientes
        grads = []
        for i in range(len(self.thetas)):
            reg_gradient = self._regularizationL2Gradient(self.thetas[i], lambda_, self._size(activations[i]))
            if i == (len(self.thetas) - 1):
                grad = np.dot(deltas[i].T, activations[i]) / self._size(activations[i]) + reg_gradient
            else:
                grad = np.dot(deltas[i][:,1:].T, activations[i]) / self._size(activations[i]) + reg_gradient
            #grad[:,1:] = grad[:,1:] + reg_gradient
            grads.append(grad)
        
                #regularizacion

        J = self.compute_cost(activations[-1], y, lambda_)
        return J, grads
    
    """
    Compute L2 regularization gradient

    Args:
        theta (array_like): a theta matrix to calculate the regularization.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 Gradient value
    """
    def _regularizationL2Gradient(self, theta, lambda_, m):
        # Calculamos la regularización L2 y la devolvemos, excluyendo el bias term
        reg_gradient_final = (lambda_ / m) * np.hstack([np.zeros((theta.shape[0], 1)), theta[:, 1:]])
        return reg_gradient_final
        
    
    
    """
    Compute L2 regularization cost

    Args:
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 cost value
    """

    def _regularizationL2Cost(self, m, lambda_):
        # Calculamos la regularización L2 y la devolvemos
        reg_cost = sum(np.sum(np.square(theta[:, 1:])) for theta in self.thetas)
        reg_cost_final = (lambda_ * reg_cost) / (2 * m)
        return reg_cost_final

    
    
    def backpropagation(self, x, y, alpha, lambda_, numIte, verbose=0):
        Jhistory = []
        for i in range(numIte):
            J, grads = self.compute_gradients(x, y, lambda_)
            for j in range(len(self.thetas)):
                self.thetas[j] -= alpha * grads[j]
            Jhistory.append(J)

            if verbose > 0:
                if i % verbose == 0 or i == (numIte - 1):
                    print(f"Iteration {(i + 1):6}: Cost {float(J):8.4f}")

        return Jhistory