import numpy as np
import math

class MLP:

    # def __init__(self, inputSize, hiddenLayers, outputSize, seed=0, epislom = 0.12):
    
    #     np.random.seed(seed)
        
    #     # Guardar parámetros de la estructura
    #     self.numHiddenLayers = len(hiddenLayers)    # Numero de capas OCULTAS
    #     self.numLayers = len(hiddenLayers) + 2      # Numero de capas TOTALES
    #     self.theta = []                             # Contenido de las capas
    #     #print(self.numHiddenLayers)

    #     # Concatenamos lista de entrada con
    #     self.layers = [inputSize] + hiddenLayers + [outputSize]
        
    #     # Inicializar pesos para las capas ocultas
    #     # Se sigue el metodo descrito con epsilon [-0.12, 0.12]
    #     for i in range(self.numLayers-1):
    #         theta = np.random.uniform(-epislom, epislom, 
    #                                    (self.layers[i+1], self.layers[i] + 1))
    #         self.theta.append(theta)
        
    #     # Conectamos la capa final fuera del bucle
    #     theta = np.random.uniform(-epislom, epislom, (outputSize, self.layers[-2] + 1))
    #     self.theta.append(theta)
        
    #     # for i, theta in enumerate(self.theta):
    #     #     print(f"Theta[{i}] shape: {theta}")
        
    #     # print(self.layers)

    def __init__(self, inputLayer, hiddenLayers, outputLayer, seed=0, epislom=0.12):
        np.random.seed(seed)
        self.layers = [inputLayer] + hiddenLayers + [outputLayer]
        self.theta = []

        for i in range(len(self.layers) - 1):
            # Inicialización de los pesos de forma aleatoria
            self.theta.append(np.random.rand(self.layers[i + 1], self.layers[i] + 1) * 2 * epislom - epislom)


    """
    Reset the theta matrix created in the constructor by both theta matrix manualy loaded.

    Args:
        theta1 (array_like): Weights for the first layer in the neural network.
        theta2 (array_like): Weights for the second layer in the neural network.
    """

    def new_trained(self,theta1,theta2):
        # Carga manual de red ya entrenada en un archivo
        self.theta1 = theta1
        self.theta2 = theta2
        
    """
    Num elements in the training data. (private)

    Args:
        x (array_like): input data. 
    """
    def _size(self,x):
        return x.shape[0]
    
    """
    Computes de sigmoid function of z (private)

    Args:
        z (array_like): activation signal received by the layer.
    """
    def _sigmoid(self,z):
        return 1 / (1 + np.exp(-z))

    """
    Computes de sigmoid derivation of de activation (private)

    Args:
        a (array_like): activation received by the layer.
    """   
    def _sigmoidPrime(self, a):
        return a * (1 - a)

    """
    Run the feedwordwar neural network step

    Args:
        x (array_like): input of the neural network.

	Return 
	------
	a1,a2,a3 (array_like): activation functions of each layers
    z2,z3 (array_like): signal fuction of two last layers
    """
    # def feedforward(self,x):

    #     # Preparacion de la capa inicial de la red
    #     # En nuestro caso esto se corresponde a 400 pixeles de la imagen y uno como sesgo adicional
    #     m = self._size(x)
    #     activaciones = [np.hstack([np.ones((m, 1)), x])]	# Adicion de neurona de sesgo (Columna de 1s)

    #     for i in range(len(self.theta)):    # Itera por la lista
    #         z = np.dot(activaciones[-1], self.theta[i].T)    # Activación de la primera capa
    #         #print(activaciones[i].shape,self.theta[i].T)
    #         a = self._sigmoid(z)	                        # Normalizacion de la capa oculta
    #         if i < len(self.theta) - 1:     # Solo añadimos sesgo en capas ocultas
    #             a = np.hstack([np.ones((a.shape[0], 1)), a])	# Adicion de neurona de sesgo
    #         activaciones.append(a)

    #     return activaciones # devolvemos a parte de las activaciones

    def feedforward(self, x):
        m = self._size(x)
        activaciones = [np.hstack([np.ones((m, 1)), x])]  # Añade bias a la entrada

        for i in range(len(self.theta)):
            z = np.dot(activaciones[i], self.theta[i].T)
            a = self._sigmoid(z)
            if i < len(self.theta) - 1:  # Añade bias solo a capas ocultas
                a = np.hstack([np.ones((a.shape[0], 1)), a])
            activaciones.append(a)
        return activaciones

    """
    Computes only the cost of a previously generated output (private)

    Args:
        yPrime (array_like): output generated by neural network.
        y (array_like): output from the dataset
        lambda_ (scalar): regularization parameter

	Return 
	------
	J (scalar): the cost.
    """
    def compute_cost(self, yPrime, y, lambda_): # es una función interna por eso empieza por _
        
        m = self._size(y)  # Número de ejemplos
        # Costo sin regularización
        J = (-1 / m) * np.sum(y * np.log(yPrime) + (1 - y) * np.log(1 - yPrime))
        
        # Regularización L2 (excluyendo el sesgo)
        cost_reg = self._regularizationL2Cost(m, lambda_)
        
        return J + cost_reg
    

    """
    Get the class with highest activation value

    Args:
        a3 (array_like): output generated by neural network.

	Return 
	------
	p (scalar): the class index with the highest activation value.
    """
    def predict(self, X):
        
        activaciones = self.feedforward(X)
        prediccion = np.argmax(activaciones[-1],axis=1)

        return prediccion
    

    """
    Compute the gradients of both theta matrix parámeters and cost J

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        lambda_ (scalar): regularization.

	Return 
	------
	J: cost
    grad1, grad2: the gradient matrix (same shape than theta1 and theta2)
    """
    #def compute_gradients(self, x, y, lambda_):

        # m = self._size(x)

        # # propagacion inicial de la red
        # activaciones = self.feedforward(x)
        # numLayers = len(activaciones)
        # J = self.compute_cost(activaciones[-1], y, lambda_) # Computecost de la última activacion
        # # Por lo visto en Python el indice -1 es el ultimo
        # # Es casi como si fuese un lenguaje hecho para humanos

        # # Inicilización de lista a None con tamaño de las capas ocultas
        # delta = [None] * (numLayers)

        # # Calcula el error en  el resultado de la última capa
        # # Esto es la salida de la red
        # delta[-1] = (activaciones[-1] - y) * self._sigmoidPrime(activaciones[-1])

        # # Aplica las correciones a las capas desde la ultima a la primera
        # # Comenzamos en la capa -2 (la -1 es el resultado) y recorremos de manera inversa
        # # Eliminamos el sesgo de la multiplicación y nos aseguramos de usar una traspuesta porque lo hacemos de atrás hacia delante
        # # Excluimos la capa 0 que es la entrada
        # for i in range(numLayers - 2, 0, -1):
        #     print(delta[i+1].shape, self.theta[i-1].T.shape)
        #     delta[i] = np.dot (delta[i + 1], self.theta[i - 1].T) * self._sigmoidPrime(activaciones[i])
        # # Basicamente calculamos el error de una capa oculta con su matriz de pesos y la derivada de su funcion de activacion

        # grads = []
        # for i in range(self.numLayers - 1):
        #     grad = np.dot(delta[i+1].T, activaciones[i]) / m
        #     #grad[:, 1:] += (lambda_ / m) * self.theta[i][:, 1:] #Elimina el sesgo de los calculos
        #     grad += self._regularizationL2Gradient(self.theta[i], lambda_, m)
        #     grads.append(grad)

        # return J, grads
         
    def compute_gradients(self, x, y, lambda_):
        activations = self.feedforward(x)
        deltas = [activations[-1] - y]

        # Backpropagation
        for i in range(len(self.theta) - 1, 0, -1):
            delta = np.dot(deltas[0], self.theta[i][:, 1:]) * self._sigmoidPrime(activations[i][:, 1:])
            deltas.insert(0, delta)

        # Añadimos la regularizacion L2 a los gradientes
        grads = []
        for i in range(len(self.theta)):
            reg_gradient = self._regularizationL2Gradient(self.theta[i], lambda_, self._size(activations[i]))
            grad = np.dot(deltas[i].T, activations[i]) / self._size(activations[i]) + reg_gradient
            grads.append(grad)

        J = self.compute_cost(activations[-1], y, lambda_)
        return J, grads
    
    """
    Compute L2 regularization gradient

    Args:
        theta (array_like): a theta matrix to calculate the regularization.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 Gradient value
    """
    def _regularizationL2Gradient(self, theta, lambda_, m):
        
        reg_gradient = (lambda_ / m) * theta.copy()
        
        # Excluimos el término de sesgo(???)
        reg_gradient[:, 0] = 0
        
        return reg_gradient
    
    
    """
    Compute L2 regularization cost

    Args:
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 cost value
    """

    def _regularizationL2Cost(self, m, lambda_):

        reg = 0
        # Sumatorio de los cuadrados de todas las capas sin el sesgo
        for theta in self.theta:
            reg += np.sum(theta[:, 1:] ** 2)
        # La propia regularizacion
        reg = (lambda_ / (2 * m)) * reg
        return reg

        # # Calclamos la regularización sin el sesgo
        # return (lambda_ / (2 * m)) * (np.sum(np.square(self.theta1[:, 1:])) + np.sum(np.square(self.theta2[:, 1:])))
        
    
    
    def backpropagation(self, x, y, alpha, lambda_, numIte, verbose=0):
        
        Jhistory = []
        
        # Generaciones
        for i in range(numIte):
            
            # Calculamos gradiente por cada iteración
            J, grads = self.compute_gradients(x,y,lambda_)

            # Actualiza pesos de todas las capas
            for l in range(len(self.theta)):
                self.theta[l] = self.theta[l] - alpha * grads[l]
            
            Jhistory.append(J)
            print(J)

            if verbose > 0 :
                if i % verbose == 0 or i == (numIte-1):
                    print(f"Iteration {(i+1):6}: Cost {float(J):8.4f}   ")
        
        return Jhistory
    


"""
target_gradient funcitón of gradient test 1
"""
def target_gradient(input_layer_size,hidden_layer_size,num_labels,x,y,reg_param):
    mlp = MLP(input_layer_size,hidden_layer_size,num_labels)
    J, grad1, grad2 = mlp.compute_gradients(x,y,reg_param)
    return J, grad1, grad2, mlp.theta1, mlp.theta2


"""
costNN funcitón of gradient test 1
"""
def costNN(Theta1, Theta2,x, ys, reg_param):
    mlp = MLP(x.shape[1],1, ys.shape[1])
    mlp.new_trained(Theta1,Theta2)
    J, grad1, grad2 = mlp.compute_gradients(x,ys,reg_param)
    return J, grad1, grad2


"""
mlp_backprop_predict 2 to be execute test 2
"""
def MLP_backprop_predict(X_train,y_train, X_test, alpha, lambda_, num_ite, verbose):
    mlp = MLP(X_train.shape[1],25,y_train.shape[1])
    Jhistory = mlp.backpropagation(X_train,y_train,alpha,lambda_,num_ite,verbose)
    a3= mlp.feedforward(X_test)[2]
    y_pred=mlp.predict(a3)
    return y_pred